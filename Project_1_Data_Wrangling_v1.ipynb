{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 1:Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import collections\n",
    "import shutil\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure DataFrame display settings\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the output directory for the pre-processed files\n",
    "dir_outfiles = 'C:/Users/Jonathon.Poage/Desktop/Springboard/Capstone_Documentation/Project_1/Data_Files/preprocessed_data/'\n",
    "\n",
    "# if the output directory exists, delete it\n",
    "if os.path.exists(dir_outfiles):\n",
    "    shutil.rmtree(dir_outfiles)\n",
    "\n",
    "# make the output directory\n",
    "os.makedirs(dir_outfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the pathname to the data files\n",
    "dir_flight_data = 'C:/Users/Jonathon.Poage/Desktop/Springboard/Capstone_Documentation/Project_1/Data_Files/flight_performance_data/'\n",
    "\n",
    "# make the list of filenames\n",
    "filepaths_flight_data = glob(dir_flight_data + '*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of airport codes: top 10 busiest airports\n",
    "airport_codes = ['ATL', 'LAX', 'ORD', 'DFW', 'JFK',\n",
    "                 'DEN', 'SFO', 'LAS', 'SEA', 'MIA']\n",
    "\n",
    "# list of airline codes: top 8 largest airlines\n",
    "carrier_codes = ['WN', 'DL', 'AA', 'UA',\n",
    "                 'B6', 'AS', 'NK', 'F9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idendify the date column\n",
    "datecol = ['FL_DATE']\n",
    "\n",
    "# dict of timezones for airports\n",
    "airport_tzs = {'ATL':'US/Eastern',\n",
    "               'LAX':'US/Pacific',\n",
    "               'ORD':'US/Central',\n",
    "               'DFW':'US/Central',\n",
    "               'JFK':'US/Eastern',\n",
    "               'DEN':'US/Mountain',\n",
    "               'SFO':'US/Pacific',\n",
    "               'LAS':'US/Pacific',\n",
    "               'SEA':'US/Pacific',\n",
    "               'MIA':'US/Eastern'}\n",
    "\n",
    "# set dtypes for the input data\n",
    "dict_dtypes_flight_data = {'CANCELLED':bool, 'DIVERTED':bool,\n",
    "                           'CRS_DEP_TIME':str, 'DEP_TIME':str,\n",
    "                           'WHEELS_OFF':str, 'WHEELS_ON':str,\n",
    "                           'CRS_ARR_TIME':str, 'ARR_TIME':str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the start time for the cell execution\n",
    "print(pd.Timestamp.now())\n",
    "\n",
    "# read the data into a list of daframes, and preprocess the dataframes\n",
    "dfs_flight_data = []\n",
    "for f in filepaths_flight_data:\n",
    "    df = pd.read_csv(f,\n",
    "                     dtype=dict_dtypes_flight_data)\n",
    "    \n",
    "    # filter the dataframe\n",
    "    df = df[df.OP_UNIQUE_CARRIER.isin(carrier_codes)\n",
    "          & df.ORIGIN.isin(airport_codes)\n",
    "          & df.DEP_DELAY.notnull()\n",
    "          & df.FL_DATE.notnull()\n",
    "          & (df.CANCELLED == False)]\n",
    "    \n",
    "    # create column with timestamps for CRS departure times\n",
    "    df['crs_dep_ts'] = pd.to_datetime(df['FL_DATE'] + ' ' + df['CRS_DEP_TIME'], errors='coerce', format='%Y-%m-%d %H%M')\n",
    "\n",
    "    # add a column with timezone info\n",
    "    df['origin_timezone'] = df.ORIGIN.map(airport_tzs)\n",
    "    \n",
    "    # create column with timestamps for actual departure times\n",
    "    df['dep_ts'] = df['crs_dep_ts'] + pd.to_timedelta(df['DEP_DELAY'], unit='m', errors='coerce')\n",
    "    \n",
    "    # create column with timestamps for wheels off times\n",
    "    df['wheels_off_ts'] = df['dep_ts'] + pd.to_timedelta(df['TAXI_OUT'], unit='m', errors='coerce')\n",
    "    \n",
    "    # filter the dataframe based on timestamp null values\n",
    "    df = df[df.wheels_off_ts.notnull()\n",
    "          & df.dep_ts.notnull()\n",
    "          & df.crs_dep_ts.notnull()]\n",
    "    \n",
    "    # append the dataframe\n",
    "    dfs_flight_data.append(df)\n",
    "\n",
    "# concatenate the pre-processed dataframes\n",
    "df_f = pd.concat(dfs_flight_data)\n",
    "\n",
    "# print the end time for the cell execution\n",
    "print(pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of unwanted columns\n",
    "df_f.drop(['FL_DATE',\n",
    "           'OP_CARRIER_AIRLINE_ID',\n",
    "           'TAIL_NUM',\n",
    "           'ORIGIN_AIRPORT_ID',\n",
    "           'ORIGIN_AIRPORT_SEQ_ID',\n",
    "           'ORIGIN_CITY_MARKET_ID',\n",
    "           'DEST_AIRPORT_ID',\n",
    "           'DEST_AIRPORT_SEQ_ID', \n",
    "           'DEST_CITY_MARKET_ID',\n",
    "           'CRS_DEP_TIME',\n",
    "           'DEP_TIME',\n",
    "           'WHEELS_OFF',\n",
    "           'WHEELS_ON',\n",
    "           'CRS_ARR_TIME',\n",
    "           'ARR_TIME',\n",
    "           'ARR_TIME_BLK',\n",
    "           'Unnamed: 38'],\n",
    "           axis='columns',\n",
    "           inplace=True)\n",
    "\n",
    "# reset the index\n",
    "df_f.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the pathname to the data files\n",
    "dir_weather_data = 'C:/Users/Jonathon.Poage/Desktop/Springboard/Capstone_Documentation/Project_1/Data_Files/weather_data/'\n",
    "\n",
    "# make the list of filenames\n",
    "filepaths_weather_data = glob(dir_weather_data + '*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify the timestamp columns\n",
    "ts_cols_weather = ['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the start time for the cell execution\n",
    "print(pd.Timestamp.now())\n",
    "\n",
    "# read the data into a list of daframes, and preprocess the dataframes\n",
    "dfs_weather_data = []\n",
    "for fw in filepaths_weather_data:\n",
    "    dfw = pd.read_csv(fw,\n",
    "                      skipinitialspace=True,\n",
    "                      parse_dates=ts_cols_weather,\n",
    "                      na_values='M')\n",
    "    \n",
    "    # filter the dataframe\n",
    "    dfw = dfw[dfw.valid.notnull()\n",
    "            & dfw.station.isin(airport_codes)]\n",
    "    \n",
    "    # filter out an anomalous data point\n",
    "    dfw = dfw[~((dfw.station == 'DEN')\n",
    "                & (dfw.valid == pd.to_datetime('2017-05-11 14:53:00')))]\n",
    "    \n",
    "    # convert wind directions of 360 to 0\n",
    "    dfw.loc[(dfw.drct == 360), 'drct'] = 0\n",
    "    \n",
    "    # add a column with timezone info\n",
    "    dfw['station_timezone'] = dfw.station.map(airport_tzs)\n",
    "    \n",
    "    # append the list of dataframes\n",
    "    dfs_weather_data.append(dfw)\n",
    "    \n",
    "# concatenate the pre-processed dataframes\n",
    "df_w = pd.concat(dfs_weather_data)\n",
    "\n",
    "# print the end time for the cell execution\n",
    "print(pd.Timestamp.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unecessary columns\n",
    "df_w.drop(['metar'],\n",
    "          axis='columns',\n",
    "          inplace=True)\n",
    "\n",
    "# reset the index\n",
    "df_w.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a list of merged dataframes\n",
    "dfs_merged = []\n",
    "\n",
    "# loop over airport codes\n",
    "for c in airport_codes:\n",
    "    # slice by airport and sort by time\n",
    "    left = df_f[df_f.ORIGIN == c].sort_values('crs_dep_ts')\n",
    "    right = df_w[df_w.station == c].sort_values('valid')\n",
    "    \n",
    "    # merge left and right.  left-join, 1 hour window \"backward\" search.\n",
    "    # NOTE: I can't use \"forward\" or \"nearest\" because my pandas version is outdated\n",
    "    merged_lr = pd.merge_asof(left,\n",
    "                              right,\n",
    "                              left_on='crs_dep_ts',\n",
    "                              right_on='valid',\n",
    "                              tolerance=pd.to_timedelta('1 hours'))\n",
    "    \n",
    "    # filter out rows without a right side match\n",
    "    merged_lr = merged_lr[merged_lr.valid.notnull()]\n",
    "    \n",
    "    # append the list of merged dataframes\n",
    "    dfs_merged.append(merged_lr)\n",
    "    \n",
    "# concatenate the merged dataframes while resetting the index\n",
    "df_m = pd.concat(dfs_merged,\n",
    "                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the output filepath\n",
    "name_outfile_m = 'merged_flight_weather_data_preprocessed.csv'\n",
    "filepath_outfile_m = dir_outfiles + name_outfile_m\n",
    "\n",
    "# write the concatenated dataframe to a csv file\n",
    "df_m.to_csv(filepath_outfile_m, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
